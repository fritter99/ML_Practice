{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_DynamicProgram.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGQHtePDX6MQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tcq--EM1S4zx",
        "colab_type": "text"
      },
      "source": [
        "## Policy Iteration\n",
        "- Policy Evaluation\n",
        "- Improve Policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXMontTjXi3k",
        "colab_type": "text"
      },
      "source": [
        "### 初始化环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHkdePu9XTdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "environment = gym.make('FrozenLake-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysMPd8qUh_5f",
        "colab_type": "code",
        "outputId": "9eee139e-7ab6-4680-e558-de60b0c03cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# 打印环境\n",
        "environment.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgM6DFbHAmbn",
        "colab_type": "code",
        "outputId": "f5a267f8-eadd-410e-e253-fb3eec00f2a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# 查看从一个state到new state的概率\n",
        "# 从state=6开始(也就是从起点开始), 进行action=1(向下走), 会有可能向左或右走.\n",
        "# LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
        "\n",
        "state = 6\n",
        "action = 1\n",
        "environment.P[state][action]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 5, 0.0, True),\n",
              " (0.3333333333333333, 10, 0.0, False),\n",
              " (0.3333333333333333, 7, 0.0, True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXGG3-wgHPXC",
        "colab_type": "text"
      },
      "source": [
        "### Policy Evaluation\n",
        "\n",
        "首先我们定义用来衡量一个policy好坏的函数."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YXJHtF1L3lY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_evaluation(policy, environment, discount_factor=0.9, theta=1e-9, max_iterations=1000):\n",
        "    \"\"\"\n",
        "    Evaluate a policy given a deterministic environment.\n",
        "\n",
        "    PARAMETERS\n",
        "    ----------\n",
        "    policy: nS*nA的矩阵, 每一个值代表在这个state采取action的概率\n",
        "    environment: gym提供的环境\n",
        "    discount_factor: 折扣因子\n",
        "    theta: 判断是否收敛, 如果value function里面的值改变较小, 则停止迭代\n",
        "    max_iterations: 最大迭代次数\n",
        "                    \n",
        "    RETURNS\n",
        "    -------\n",
        "    V: 这个policy对应的value function\n",
        "    \"\"\"\n",
        "    # 记录迭代次数\n",
        "    evaluation_iterations = 1\n",
        "    vList = [0] # 记录每次迭代前后v平均值的变化\n",
        "\n",
        "    # 初始化value function向量, 也就是每个state到游戏结束可能获得的累计reward\n",
        "    V = np.zeros(environment.nS)\n",
        "\n",
        "    for i in range(int(max_iterations)):\n",
        "        # 记录两次迭代value function改变的值, 用来判断是否收敛\n",
        "        delta = 0\n",
        "        \n",
        "        for state in range(environment.nS): # 对所有的状态进行遍历\t\t\t\n",
        "            # 记录当前state的value\n",
        "            v = 0\n",
        "            \n",
        "            # 当前state下所有能做的action和做的概率\n",
        "            for action, action_probability in enumerate(policy[state]):\n",
        "                # 采取相应的action之后, 下一步的新的state出现的概率\n",
        "                for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
        "                    # 这里修改一下原始reward的值, 每走一步reward减一, 终点reward是10\n",
        "                    if reward == 0:\n",
        "                        reward = reward - 1 # 每走一步reward都是-1\n",
        "                    elif reward == 1:\n",
        "                        reward = 10\n",
        "                    # 计算value值\n",
        "                    v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
        "                \n",
        "            # 保存一个更新前后最大的差距\n",
        "            delta = max(delta, abs(V[state] - v))\n",
        "            \n",
        "            # 更新state value\n",
        "            V[state] = v\n",
        "        \n",
        "        # 计算前后差值, 用来绘图\n",
        "        vList.append(np.abs(np.mean(V)))\n",
        "        \n",
        "        # 更新迭代次数\n",
        "        evaluation_iterations += 1\n",
        "\n",
        "        # 若收敛, 则早停止\n",
        "        if(delta < theta):\n",
        "            # print('Policy evaluated in %d iterations' % evaluation_iterations)\n",
        "            vdeltaList = np.array(vList[1:]) - np.array(vList[:-1])\n",
        "            return V, vdeltaList\n",
        "    \n",
        "    vdeltaList = np.array(vList[1:]) - np.array(vList[:-1])\n",
        "    return V, vdeltaList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cztRXVArYcb_",
        "colab_type": "text"
      },
      "source": [
        "### Policy Iteration\n",
        "\n",
        "接着我们进行policy的迭代. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUCHJZVShjIs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "da4c390e-9a70-4b30-c690-f1e168dcacd3"
      },
      "source": [
        "# LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
        "environment.P[0][3]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVPssjslmUdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_lookahead(environment, state, V, discount_factor):\n",
        "    # Create a vector of dimensionality same as the number of actions\n",
        "    action_values = np.zeros(environment.nA)\n",
        "\n",
        "    for action in range(environment.nA):\n",
        "        # 采取同一个action, 达到不同的state\n",
        "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
        "            if reward == 0:\n",
        "                reward = -1\n",
        "            elif reward == 1:\n",
        "                reward = 10\n",
        "            action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
        "    return action_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KePFUA2pYt1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
        "    # 初始化一个策略, 每一个action都是平均的\n",
        "    policy = np.ones((environment.nS, environment.nA)) / environment.nA\n",
        "    print('Step:0;\\n V:\\n{};\\n Policy:\\n{}'.format(np.zeros(environment.nS).reshape(4,4), policy))\n",
        "    # Store the number of policies evaluated\n",
        "    evaluated_policies = 1\n",
        "\t\n",
        "    for i in range(int(max_iterations)):\t\n",
        "        # For Early Stopping\n",
        "        stable_policy = True\n",
        "        # 首先对当前策略进行评价\n",
        "        V,_ = policy_evaluation(policy, environment, discount_factor=discount_factor)\n",
        "        # 检查每一个state\n",
        "        for state in range(environment.nS):\n",
        "            # 获得当前state下最大概率的一个action\n",
        "            current_action = np.argmax(policy[state])\n",
        "            # 计算这个state出发的action value\n",
        "            action_values = one_step_lookahead(environment, state, V, discount_factor)\n",
        "            # 获得最好的动作\n",
        "            best_action = np.argmax(action_values)\n",
        "            # 如果动作还在改变, 就说明还在更新\n",
        "            if(current_action != best_action):\n",
        "                stable_policy = False\n",
        "            # 更新policy函数\n",
        "            policy[state] = np.eye(environment.nA)[best_action]\n",
        "\t\t\n",
        "        # Increment the number of policies evaluated\n",
        "        print('Step:{}, Stable Policy:{};\\n V:\\n{};\\n Policy:\\n{}'.format(evaluated_policies, stable_policy, V.reshape(4,4), policy))\n",
        "        print('='*7)\n",
        "        evaluated_policies += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if(stable_policy):\n",
        "            print('Evaluated %d policies.' % evaluated_policies)\n",
        "            return policy, V\n",
        "    return policy, V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qVupg1Y1ku",
        "colab_type": "code",
        "outputId": "5d32fb76-6173-4d99-b78d-4b9882f2bb0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "policy, V = policy_iteration(environment=environment, discount_factor=0.9, max_iterations=300)\n",
        "print(V.reshape(4,4))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:0;\n",
            " V:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]];\n",
            " Policy:\n",
            "[[0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]\n",
            " [0.25 0.25 0.25 0.25]]\n",
            "Step:1, Stable Policy:False;\n",
            " V:\n",
            "[[-9.95075012 -9.95355297 -9.88926567 -9.95469959]\n",
            " [-9.92605845 -9.99999999 -9.7103292  -9.99999999]\n",
            " [-9.79456232 -9.3663229  -8.82330857 -9.99999999]\n",
            " [-9.99999999 -8.56578645 -5.69360823 -9.99999999]];\n",
            " Policy:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "=======\n",
            "Step:2, Stable Policy:True;\n",
            " V:\n",
            "[[-9.24220004 -9.32443971 -9.18149261 -9.38611946]\n",
            " [-8.98960006 -9.99999999 -8.76570972 -9.99999999]\n",
            " [-8.40020009 -7.27753349 -6.70420647 -9.99999999]\n",
            " [-9.99999999 -5.82070508 -2.97077836 -9.99999999]];\n",
            " Policy:\n",
            "[[1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]]\n",
            "=======\n",
            "Evaluated 3 policies.\n",
            "[[-9.24220004 -9.32443971 -9.18149261 -9.38611946]\n",
            " [-8.98960006 -9.99999999 -8.76570972 -9.99999999]\n",
            " [-8.40020009 -7.27753349 -6.70420647 -9.99999999]\n",
            " [-9.99999999 -5.82070508 -2.97077836 -9.99999999]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjZmC_eDuLnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf2ee7cf-5538-4174-bb71-05980cb2a027"
      },
      "source": [
        "VE = np.array([-9.95075012, -9.95355297, -9.88926567, -9.95469959, \n",
        "        -9.92605845, -9.99999999, -9.7103292,  -9.99999999, \n",
        "        -9.79456232, -9.3663229,  -8.82330857, -9.99999999, \n",
        "        -9.99999999, -8.56578645, -5.69360823, -9.99999999])\n",
        "one_step_lookahead(environment, state=14, V=VE, discount_factor=1)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-8.69423442, -5.41979822, -5.50563893, -6.46303167])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8QAJRaWZmf0",
        "colab_type": "code",
        "outputId": "e6945dc1-0ab7-4f15-f485-498a6fea25af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "# LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3\n",
        "policy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMQy1nAbt514",
        "colab_type": "text"
      },
      "source": [
        "### 每一步的选择"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Ir8C4rfHlI",
        "colab_type": "code",
        "outputId": "6f71edef-9f49-4084-b2b0-88d80fac2487",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(np.argmax(policy[0]))\n",
        "environment.P[0][np.argmax(policy[0])]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgkDuhmUgDJM",
        "colab_type": "code",
        "outputId": "b439fe9e-809a-4758-975e-05d7a8b8e2eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(np.argmax(policy[4]))\n",
        "environment.P[4][np.argmax(policy[4])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False),\n",
              " (0.3333333333333333, 8, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAeC2Lh3gVvS",
        "colab_type": "code",
        "outputId": "4fb37694-a096-4302-c693-7c67040f574f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(np.argmax(policy[8]))\n",
        "environment.P[8][np.argmax(policy[8])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 9, 0.0, False),\n",
              " (0.3333333333333333, 4, 0.0, False),\n",
              " (0.3333333333333333, 8, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUc9-CFRgXwr",
        "colab_type": "code",
        "outputId": "9e7e8391-a233-4354-fedc-52fca8f8b34b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(np.argmax(policy[9]))\n",
        "environment.P[9][np.argmax(policy[9])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 8, 0.0, False),\n",
              " (0.3333333333333333, 13, 0.0, False),\n",
              " (0.3333333333333333, 10, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlxHojGFg8eA",
        "colab_type": "code",
        "outputId": "c5f40d60-e6ff-494e-f35b-b80300087e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(np.argmax(policy[10]))\n",
        "environment.P[10][np.argmax(policy[10])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 6, 0.0, False),\n",
              " (0.3333333333333333, 9, 0.0, False),\n",
              " (0.3333333333333333, 14, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REqi06qdhH1J",
        "colab_type": "code",
        "outputId": "7836f33d-ae4e-406c-b4ec-1521d9f9657d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "print(np.argmax(policy[13]))\n",
        "environment.P[13][np.argmax(policy[13])]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 13, 0.0, False),\n",
              " (0.3333333333333333, 14, 0.0, False),\n",
              " (0.3333333333333333, 9, 0.0, False)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awnlr8CmhM-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}